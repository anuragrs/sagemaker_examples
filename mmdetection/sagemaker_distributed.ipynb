{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MMDetection Mask-RCNN Model on Sagemaker Distributed Cluster\n",
    "\n",
    "## Motivation\n",
    "[MMDetection](https://github.com/open-mmlab/mmdetection) is a popular open-source Deep Learning framework focused on Computer Vision models and use cases. MMDetection provides to higher level APIs for model training and inference. It demonstrates [state-of-the-art benchmarks](https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo) for variety of model architecture and extensive Model Zoo.\n",
    "\n",
    "In this notebook, we will build a custom training container with MMdetection library and then train Mask-RCNN model from scratch on [COCO2017 dataset](https://cocodataset.org/#home) using Sagemaker distributed [training feature](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) in order to reduce training time.\n",
    "\n",
    "### Preconditions\n",
    "- To execute this notebook, you will need to have COCO 2017 training and validation datasets uploaded to S3 bucket available for Amazon Sagemaker service.\n",
    "\n",
    "\n",
    "## Building Training Container\n",
    "\n",
    "Amazon Sagemaker allows to BYO containers for training, data processing, and inference. In our case, we need to build custom training container which will be pushed to your AWS account [ECR service](https://aws.amazon.com/ecr/). \n",
    "\n",
    "For this, we need to login to public ECR with Sagemaker base images and private ECR reposity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "container = \"mzanur-mmdetection-training\" # your container name\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-564829616587'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let review training container:\n",
    "- use Sagemaker PyTorch container as base image;\n",
    "- install Pytorch libraries and MMdetection dependencies;\n",
    "- build MMDetection from sources;\n",
    "- configure Sagemaker env variables, specifically, what script to use at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Build an image of mmdetection that can do distributing training on Amazon Sagemaker \u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# using Sagemaker PyTorch container as base image\u001b[39;49;00m\n",
      "\u001b[37m# from https://github.com/aws/sagemaker-pytorch-container\u001b[39;49;00m\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mUBUNTU\u001b[39;49;00m=\u001b[33m\"18.04\"\u001b[39;49;00m\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mPYTORCH\u001b[39;49;00m=\u001b[33m\"1.7.1\"\u001b[39;49;00m\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mCUDA\u001b[39;49;00m=\u001b[33m\"110\"\u001b[39;49;00m\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mREGION\u001b[39;49;00m=\u001b[33m\"us-east-1\"\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-training:${PYTORCH}-gpu-py36-cu${CUDA}-ubuntu${UBUNTU}\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# BASIC SETUP ##############\u001b[39;49;00m\n",
      " RUN apt-get update\n",
      " RUN apt-get install -y curl git\n",
      " RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 \u001b[33m\\\u001b[39;49;00m\n",
      "    && apt-get clean \u001b[33m\\\u001b[39;49;00m\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "\u001b[37m############# Use pre-built binaries Pytorch default install ############\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[31mtorchvision\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.8.2 \u001b[31mtorchaudio\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.7.2\n",
      "\n",
      "\u001b[37m############# mmdetection section ##############\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\u001b[37m#./1.3.0/torch1.7.0/cu110/mmcv_full-1.3.0+torch1.7.0+cu110-cp36-cp36m-manylinux1_x86_64.whl\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install mmcv-full==\u001b[34m1\u001b[39;49;00m.3.0 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html\n",
      "\u001b[34mRUN\u001b[39;49;00m conda clean --all\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone https://github.com/open-mmlab/mmdetection.git mmdetection\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=TRUE\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONDONTWRITEBYTECODE\u001b[39;49;00m=TRUE\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFORCE_CUDA\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mTORCH_CUDA_ARCH_LIST\u001b[39;49;00m=\u001b[33m\"Pascal;Volta;Turing\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m MMDETECTION /opt/ml/code/mmdetection\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m mmdetection/ && \u001b[33m\\\u001b[39;49;00m\n",
      "    pip install -r /opt/ml/code/mmdetection/requirements/build.txt && \u001b[33m\\\u001b[39;49;00m\n",
      "    pip install --no-cache-dir -e .\n",
      "\n",
      "\u001b[37m############# Copy workaround script for incorrect hostname - unsure why we need this but generally present in SM examples #############\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m lib/changehostname.c /\n",
      "\u001b[34mCOPY\u001b[39;49;00m lib/start_with_right_hostname.sh /usr/local/bin/start_with_right_hostname.sh\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /usr/local/bin/start_with_right_hostname.sh\n",
      "\n",
      "\u001b[37m############# SageMaker section ##############\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training/mmdetection_train.py /opt/ml/code\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM mmdetection_train.py\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Starts PyTorch distributed framework\u001b[39;49;00m\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"bash\"\u001b[39;49;00m, \u001b[33m\"-m\"\u001b[39;49;00m, \u001b[33m\"start_with_right_hostname.sh\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Next, we build and push custom training container to private ECR\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in region us-east-1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  397.8kB\n",
      "Step 1/26 : ARG UBUNTU=\"16.04\"\n",
      "Step 2/26 : ARG PYTORCH=\"1.6.0\"\n",
      "Step 3/26 : ARG CUDA=\"101\"\n",
      "Step 4/26 : ARG REGION=\"us-east-1\"\n",
      "Step 5/26 : FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-training:${PYTORCH}-gpu-py36-cu${CUDA}-ubuntu${UBUNTU}\n",
      "1.6.0-gpu-py36-cu101-ubuntu16.04: Pulling from pytorch-training\n",
      "\n",
      "\u001b[1Ba89234b4: Pulling fs layer \n",
      "\u001b[1B26c6b9c9: Pulling fs layer \n",
      "\u001b[1Bbf18aa40: Pulling fs layer \n",
      "\u001b[1Bc688ebe3: Pulling fs layer \n",
      "\u001b[1Bd5861307: Pulling fs layer \n",
      "\u001b[1B27b8f0ff: Pulling fs layer \n",
      "\u001b[1B81630d15: Pulling fs layer \n",
      "\u001b[1Be18332c4: Pulling fs layer \n",
      "\u001b[1Bdfb2533b: Pulling fs layer \n",
      "\u001b[1B60a54609: Pulling fs layer \n",
      "\u001b[1Bc09e1537: Pulling fs layer \n",
      "\u001b[1B7b98fd72: Pulling fs layer \n",
      "\u001b[1B45e223e8: Pulling fs layer \n",
      "\u001b[1B55fe5c2c: Pulling fs layer \n",
      "\u001b[10B7b8f0ff: Waiting fs layer \n",
      "\u001b[1B0504e048: Pulling fs layer \n",
      "\u001b[1Bc465adf2: Pulling fs layer \n",
      "\u001b[1B90151428: Pulling fs layer \n",
      "\u001b[1Bf3e5d45d: Pulling fs layer \n",
      "\u001b[1B116f6a25: Pulling fs layer \n",
      "\u001b[15B1630d15: Waiting fs layer \n",
      "\u001b[1B01d38313: Pulling fs layer \n",
      "\u001b[13B09e1537: Waiting fs layer \n",
      "\u001b[13Bb98fd72: Waiting fs layer \n",
      "\u001b[1Ba32b3c3e: Pulling fs layer \n",
      "\u001b[19B18332c4: Waiting fs layer \n",
      "\u001b[1Be49e2a2e: Pulling fs layer \n",
      "\u001b[1B4d9b85d6: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:3b8840ca36163a85bc94e6b8caac0934bd08196dd1a1a49c39c70b10709f26012K\u001b[21A\u001b[2K\u001b[29A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[29A\u001b[2K\u001b[20A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[29A\u001b[2K\u001b[19A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[29A\u001b[2K\u001b[18A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[29A\u001b[2K\u001b[18A\u001b[2K\u001b[29A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[29A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[28A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[25A\u001b[2K\u001b[20A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[22A\u001b[2K\u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2KDownloading  319.3MB/532.4MB\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[17A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[17A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[22A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[7A\u001b[2K\u001b[22A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[7A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[21A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[6A\u001b[2K\u001b[13A\u001b[2K\u001b[6A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[6A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[5A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[5A\u001b[2K\u001b[13A\u001b[2K\u001b[5A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[13A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[20A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> cfe08bbde038\n",
      "Step 6/26 : RUN apt-get update\n",
      " ---> Running in bcfb6e97ac56\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]\n",
      "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [697 B]\n",
      "Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release [564 B]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [836 B]\n",
      "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg [833 B]\n",
      "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages\n",
      "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Packages [122 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [567 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [2002 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [15.9 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [984 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [8820 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [2511 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [16.4 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [1540 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [26.2 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [10.9 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [12.6 kB]\n",
      "Fetched 20.0 MB in 2s (8229 kB/s)\n",
      "Reading package lists...\n",
      "Removing intermediate container bcfb6e97ac56\n",
      " ---> 5945d64e7569\n",
      "Step 7/26 : RUN apt-get install -y curl git\n",
      " ---> Running in 74928d9c606a\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-curand-dev-10-1 cuda-cusparse-dev-10-1\n",
      "Use 'apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  libcurl3-gnutls\n",
      "Suggested packages:\n",
      "  gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email\n",
      "  git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn\n",
      "Recommended packages:\n",
      "  less rsync\n",
      "The following packages will be upgraded:\n",
      "  curl git libcurl3-gnutls\n",
      "3 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
      "Need to get 3511 kB of archives.\n",
      "After this operation, 16.4 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 curl amd64 7.47.0-1ubuntu2.19 [139 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcurl3-gnutls amd64 7.47.0-1ubuntu2.19 [189 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git amd64 1:2.7.4-0ubuntu1.10 [3183 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 3511 kB in 0s (4166 kB/s)\n",
      "(Reading database ... 51345 files and directories currently installed.)\n",
      "Preparing to unpack .../curl_7.47.0-1ubuntu2.19_amd64.deb ...\n",
      "Unpacking curl (7.47.0-1ubuntu2.19) over (7.47.0-1ubuntu2.18) ...\n",
      "Preparing to unpack .../libcurl3-gnutls_7.47.0-1ubuntu2.19_amd64.deb ...\n",
      "Unpacking libcurl3-gnutls:amd64 (7.47.0-1ubuntu2.19) over (7.47.0-1ubuntu2.18) ...\n",
      "Preparing to unpack .../git_1%3a2.7.4-0ubuntu1.10_amd64.deb ...\n",
      "Unpacking git (1:2.7.4-0ubuntu1.10) over (1:2.7.4-0ubuntu1.9) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Setting up libcurl3-gnutls:amd64 (7.47.0-1ubuntu2.19) ...\n",
      "Setting up curl (7.47.0-1ubuntu2.19) ...\n",
      "Setting up git (1:2.7.4-0ubuntu1.10) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Removing intermediate container 74928d9c606a\n",
      " ---> c15bb8a2efad\n",
      "Step 8/26 : RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6     && apt-get clean     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 472d6d94733f\n",
      "Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease\n",
      "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "libxrender-dev is already the newest version (1:0.9.9-0ubuntu1).\n",
      "git is already the newest version (1:2.7.4-0ubuntu1.10).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-curand-dev-10-1 cuda-cusparse-dev-10-1\n",
      "Use 'apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  i965-va-driver libaacs0 libass5 libasyncns0 libavc1394-0 libavcodec-ffmpeg56\n",
      "  libavdevice-ffmpeg56 libavfilter-ffmpeg5 libavformat-ffmpeg56\n",
      "  libavresample-ffmpeg2 libavutil-ffmpeg54 libbdplus0 libbluray1 libbs2b0\n",
      "  libcaca0 libcdio-cdda1 libcdio-paranoia1 libcdio13 libcrystalhd3\n",
      "  libdc1394-22 libflac8 libflite1 libgme0 libgsm1 libiec61883-0\n",
      "  libjack-jackd2-0 libmodplug1 libmp3lame0 libogg0 libopenal-data libopenal1\n",
      "  libopencv-core2.4v5 libopencv-imgproc2.4v5 libopenjpeg5 libopus0\n",
      "  liborc-0.4-0 libpostproc-ffmpeg53 libpulse0 libraw1394-11 libsamplerate0\n",
      "  libschroedinger-1.0-0 libsdl1.2debian libshine3 libslang2 libsnappy1v5\n",
      "  libsndfile1 libsodium18 libsoxr0 libspeex1 libssh-gcrypt-4\n",
      "  libswresample-ffmpeg1 libswscale-ffmpeg3 libtbb2 libtheora0 libtwolame0\n",
      "  libusb-1.0-0 libva1 libvdpau1 libvorbis0a libvorbisenc2 libwavpack1 libwebp5\n",
      "  libx264-148 libx265-79 libxcb-shape0 libxv1 libxvidcore4 libzmq5\n",
      "  libzvbi-common libzvbi0 mesa-va-drivers mesa-vdpau-drivers va-driver-all\n",
      "  vdpau-driver-all\n",
      "Suggested packages:\n",
      "  ffmpeg-doc libbluray-bdj firmware-crystalhd alsa-base jackd2 libportaudio2\n",
      "  opus-tools pulseaudio libraw1394-doc speex libfglrx-amdxvba1 libvdpau-va-gl1\n",
      "  nvidia-vdpau-driver nvidia-legacy-340xx-vdpau-driver\n",
      "Recommended packages:\n",
      "  libglib2.0-data xdg-user-dirs\n",
      "The following NEW packages will be installed:\n",
      "  ffmpeg i965-va-driver libaacs0 libass5 libasyncns0 libavc1394-0\n",
      "  libavcodec-ffmpeg56 libavdevice-ffmpeg56 libavfilter-ffmpeg5\n",
      "  libavformat-ffmpeg56 libavresample-ffmpeg2 libavutil-ffmpeg54 libbdplus0\n",
      "  libbluray1 libbs2b0 libcaca0 libcdio-cdda1 libcdio-paranoia1 libcdio13\n",
      "  libcrystalhd3 libdc1394-22 libflac8 libflite1 libgme0 libgsm1 libiec61883-0\n",
      "  libjack-jackd2-0 libmodplug1 libmp3lame0 libogg0 libopenal-data libopenal1\n",
      "  libopencv-core2.4v5 libopencv-imgproc2.4v5 libopenjpeg5 libopus0\n",
      "  liborc-0.4-0 libpostproc-ffmpeg53 libpulse0 libraw1394-11 libsamplerate0\n",
      "  libschroedinger-1.0-0 libsdl1.2debian libshine3 libslang2 libsnappy1v5\n",
      "  libsndfile1 libsodium18 libsoxr0 libspeex1 libssh-gcrypt-4\n",
      "  libswresample-ffmpeg1 libswscale-ffmpeg3 libtbb2 libtheora0 libtwolame0\n",
      "  libusb-1.0-0 libva1 libvdpau1 libvorbis0a libvorbisenc2 libwavpack1 libwebp5\n",
      "  libx264-148 libx265-79 libxcb-shape0 libxv1 libxvidcore4 libzmq5\n",
      "  libzvbi-common libzvbi0 mesa-va-drivers mesa-vdpau-drivers ninja-build\n",
      "  va-driver-all vdpau-driver-all\n",
      "The following packages will be upgraded:\n",
      "  libglib2.0-0\n",
      "1 upgraded, 76 newly installed, 0 to remove and 19 not upgraded.\n",
      "Need to get 34.6 MB of archives.\n",
      "After this operation, 130 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libglib2.0-0 amd64 2.48.2-0ubuntu4.8 [1120 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libasyncns0 amd64 0.8-5build1 [12.3 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libcrystalhd3 amd64 1:0.0~git20110715.fdd2f19-11build1 [46.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libgsm1 amd64 1.0.13-4 [27.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu xenial/main amd64 libraw1394-11 amd64 2.1.1-2 [30.6 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 libiec61883-0 amd64 1.2.0-0.2 [23.6 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu xenial/main amd64 libogg0 amd64 1.3.2-1 [17.2 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu xenial/main amd64 libsamplerate0 amd64 0.1.8-8 [937 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu xenial/main amd64 liborc-0.4-0 amd64 1:0.4.25-1 [138 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libschroedinger-1.0-0 amd64 1.0.11-2.1build1 [293 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxv1 amd64 2:1.0.10-1 [10.3 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libopenal-data all 1:1.16.0-3 [101 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libopenal1 amd64 1:1.16.0-3 [164 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libslang2 amd64 2.3.0-2ubuntu1.1 [415 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu xenial/main amd64 libusb-1.0-0 amd64 2:1.0.20-1 [42.9 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavutil-ffmpeg54 amd64 7:2.8.17-0ubuntu0.1 [166 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libmp3lame0 amd64 3.99.5+repack1-9build1 [127 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libopenjpeg5 amd64 1:1.5.2-3.1 [104 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu xenial/main amd64 libopus0 amd64 1.1.2-1ubuntu1 [159 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libshine3 amd64 3.1.0-4 [25.5 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu xenial/main amd64 libsnappy1v5 amd64 1.1.3-2 [16.0 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu xenial/main amd64 libspeex1 amd64 1.2~rc1.2-1ubuntu1 [51.3 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libsoxr0 amd64 0.1.2-1 [70.2 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libswresample-ffmpeg1 amd64 7:2.8.17-0ubuntu0.1 [51.7 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu xenial/main amd64 libtheora0 amd64 1.1.1+dfsg.1-8 [163 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libtwolame0 amd64 0.3.13-1.2 [49.2 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libva1 amd64 1.7.0-1ubuntu0.1 [45.4 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libvorbis0a amd64 1.3.5-3ubuntu0.2 [86.0 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libvorbisenc2 amd64 1.3.5-3ubuntu0.2 [70.6 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libwavpack1 amd64 4.75.2-2ubuntu0.2 [74.8 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu xenial/main amd64 libwebp5 amd64 0.4.4-1 [165 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libx264-148 amd64 2:0.148.2643+git5c65704-1 [606 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libx265-79 amd64 1.9-3 [965 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libxvidcore4 amd64 2:1.3.4-1 [206 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libzvbi-common all 0.2.35-10 [32.5 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libzvbi0 amd64 0.2.35-10 [235 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavcodec-ffmpeg56 amd64 7:2.8.17-0ubuntu0.1 [4097 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu xenial/main amd64 libavc1394-0 amd64 0.5.4-4 [16.1 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libass5 amd64 0.13.1-1 [82.6 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libbluray1 amd64 1:0.9.2-2 [127 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libgme0 amd64 0.6.0-3ubuntu0.16.04.1 [122 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libmodplug1 amd64 1:0.8.8.5-2 [153 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libssh-gcrypt-4 amd64 0.6.3-4.3ubuntu0.6 [120 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavformat-ffmpeg56 amd64 7:2.8.17-0ubuntu0.1 [810 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavresample-ffmpeg2 amd64 7:2.8.17-0ubuntu0.1 [52.0 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libbs2b0 amd64 3.1.0+dfsg-2.2 [10.5 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libflite1 amd64 2.0.0-release-1 [12.8 MB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libtbb2 amd64 4.4~20151115-0ubuntu3 [111 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libopencv-core2.4v5 amd64 2.4.9.1+dfsg-1.5ubuntu1.1 [674 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libopencv-imgproc2.4v5 amd64 2.4.9.1+dfsg-1.5ubuntu1.1 [632 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libpostproc-ffmpeg53 amd64 7:2.8.17-0ubuntu0.1 [48.9 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libswscale-ffmpeg3 amd64 7:2.8.17-0ubuntu0.1 [146 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libsodium18 amd64 1.0.8-5 [144 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libzmq5 amd64 4.1.4-7ubuntu0.1 [149 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavfilter-ffmpeg5 amd64 7:2.8.17-0ubuntu0.1 [530 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcaca0 amd64 0.99.beta19-2ubuntu0.16.04.1 [202 kB]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu xenial/main amd64 libcdio13 amd64 0.83-4.2ubuntu1 [53.0 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu xenial/main amd64 libcdio-cdda1 amd64 0.83-4.2ubuntu1 [15.7 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu xenial/main amd64 libcdio-paranoia1 amd64 0.83-4.2ubuntu1 [15.4 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libdc1394-22 amd64 2.2.4-1 [78.8 kB]\n",
      "Get:61 http://archive.ubuntu.com/ubuntu xenial/main amd64 libjack-jackd2-0 amd64 1.9.10+20150825git1ed50c92~dfsg-1ubuntu1 [140 kB]\n",
      "Get:62 http://archive.ubuntu.com/ubuntu xenial/main amd64 libflac8 amd64 1.3.1-4 [210 kB]\n",
      "Get:63 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsndfile1 amd64 1.0.25-10ubuntu0.16.04.3 [139 kB]\n",
      "Get:64 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpulse0 amd64 1:8.0-0ubuntu3.15 [252 kB]\n",
      "Get:65 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsdl1.2debian amd64 1.2.15+dfsg1-3ubuntu0.1 [168 kB]\n",
      "Get:66 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxcb-shape0 amd64 1.11.1-1ubuntu1 [5756 B]\n",
      "Get:67 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 libavdevice-ffmpeg56 amd64 7:2.8.17-0ubuntu0.1 [70.6 kB]\n",
      "Get:68 http://archive.ubuntu.com/ubuntu xenial/main amd64 libvdpau1 amd64 1.1.1-3ubuntu1 [25.5 kB]\n",
      "Get:69 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 ffmpeg amd64 7:2.8.17-0ubuntu0.1 [1289 kB]\n",
      "Get:70 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libaacs0 amd64 0.8.1-1 [47.4 kB]\n",
      "Get:71 http://archive.ubuntu.com/ubuntu xenial/universe amd64 libbdplus0 amd64 0.1.2-1 [47.1 kB]\n",
      "Get:72 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 mesa-va-drivers amd64 18.0.5-0ubuntu0~16.04.1 [1826 kB]\n",
      "Get:73 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 mesa-vdpau-drivers amd64 18.0.5-0ubuntu0~16.04.1 [1967 kB]\n",
      "Get:74 http://archive.ubuntu.com/ubuntu xenial/universe amd64 ninja-build amd64 1.5.1-0.1ubuntu1 [84.8 kB]\n",
      "Get:75 http://archive.ubuntu.com/ubuntu xenial/universe amd64 i965-va-driver amd64 1.7.0-1 [272 kB]\n",
      "Get:76 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 va-driver-all amd64 1.7.0-1ubuntu0.1 [4534 B]\n",
      "Get:77 http://archive.ubuntu.com/ubuntu xenial/main amd64 vdpau-driver-all amd64 1.1.1-3ubuntu1 [4674 B]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 34.6 MB in 2s (13.0 MB/s)\n",
      "(Reading database ... 51345 files and directories currently installed.)\n",
      "Preparing to unpack .../libglib2.0-0_2.48.2-0ubuntu4.8_amd64.deb ...\n",
      "Unpacking libglib2.0-0:amd64 (2.48.2-0ubuntu4.8) over (2.48.2-0ubuntu4.6) ...\n",
      "Selecting previously unselected package libasyncns0:amd64.\n",
      "Preparing to unpack .../libasyncns0_0.8-5build1_amd64.deb ...\n",
      "Unpacking libasyncns0:amd64 (0.8-5build1) ...\n",
      "Selecting previously unselected package libcrystalhd3:amd64.\n",
      "Preparing to unpack .../libcrystalhd3_1%3a0.0~git20110715.fdd2f19-11build1_amd64.deb ...\n",
      "Unpacking libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-11build1) ...\n",
      "Selecting previously unselected package libgsm1:amd64.\n",
      "Preparing to unpack .../libgsm1_1.0.13-4_amd64.deb ...\n",
      "Unpacking libgsm1:amd64 (1.0.13-4) ...\n",
      "Selecting previously unselected package libraw1394-11:amd64.\n",
      "Preparing to unpack .../libraw1394-11_2.1.1-2_amd64.deb ...\n",
      "Unpacking libraw1394-11:amd64 (2.1.1-2) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../libiec61883-0_1.2.0-0.2_amd64.deb ...\n",
      "Unpacking libiec61883-0:amd64 (1.2.0-0.2) ...\n",
      "Selecting previously unselected package libogg0:amd64.\n",
      "Preparing to unpack .../libogg0_1.3.2-1_amd64.deb ...\n",
      "Unpacking libogg0:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../libsamplerate0_0.1.8-8_amd64.deb ...\n",
      "Unpacking libsamplerate0:amd64 (0.1.8-8) ...\n",
      "Selecting previously unselected package liborc-0.4-0:amd64.\n",
      "Preparing to unpack .../liborc-0.4-0_1%3a0.4.25-1_amd64.deb ...\n",
      "Unpacking liborc-0.4-0:amd64 (1:0.4.25-1) ...\n",
      "Selecting previously unselected package libschroedinger-1.0-0:amd64.\n",
      "Preparing to unpack .../libschroedinger-1.0-0_1.0.11-2.1build1_amd64.deb ...\n",
      "Unpacking libschroedinger-1.0-0:amd64 (1.0.11-2.1build1) ...\n",
      "Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../libxv1_2%3a1.0.10-1_amd64.deb ...\n",
      "Unpacking libxv1:amd64 (2:1.0.10-1) ...\n",
      "Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../libopenal-data_1%3a1.16.0-3_all.deb ...\n",
      "Unpacking libopenal-data (1:1.16.0-3) ...\n",
      "Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../libopenal1_1%3a1.16.0-3_amd64.deb ...\n",
      "Unpacking libopenal1:amd64 (1:1.16.0-3) ...\n",
      "Selecting previously unselected package libslang2:amd64.\n",
      "Preparing to unpack .../libslang2_2.3.0-2ubuntu1.1_amd64.deb ...\n",
      "Unpacking libslang2:amd64 (2.3.0-2ubuntu1.1) ...\n",
      "Selecting previously unselected package libusb-1.0-0:amd64.\n",
      "Preparing to unpack .../libusb-1.0-0_2%3a1.0.20-1_amd64.deb ...\n",
      "Unpacking libusb-1.0-0:amd64 (2:1.0.20-1) ...\n",
      "Selecting previously unselected package libavutil-ffmpeg54:amd64.\n",
      "Preparing to unpack .../libavutil-ffmpeg54_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavutil-ffmpeg54:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libmp3lame0:amd64.\n",
      "Preparing to unpack .../libmp3lame0_3.99.5+repack1-9build1_amd64.deb ...\n",
      "Unpacking libmp3lame0:amd64 (3.99.5+repack1-9build1) ...\n",
      "Selecting previously unselected package libopenjpeg5:amd64.\n",
      "Preparing to unpack .../libopenjpeg5_1%3a1.5.2-3.1_amd64.deb ...\n",
      "Unpacking libopenjpeg5:amd64 (1:1.5.2-3.1) ...\n",
      "Selecting previously unselected package libopus0:amd64.\n",
      "Preparing to unpack .../libopus0_1.1.2-1ubuntu1_amd64.deb ...\n",
      "Unpacking libopus0:amd64 (1.1.2-1ubuntu1) ...\n",
      "Selecting previously unselected package libshine3:amd64.\n",
      "Preparing to unpack .../libshine3_3.1.0-4_amd64.deb ...\n",
      "Unpacking libshine3:amd64 (3.1.0-4) ...\n",
      "Selecting previously unselected package libsnappy1v5:amd64.\n",
      "Preparing to unpack .../libsnappy1v5_1.1.3-2_amd64.deb ...\n",
      "Unpacking libsnappy1v5:amd64 (1.1.3-2) ...\n",
      "Selecting previously unselected package libspeex1:amd64.\n",
      "Preparing to unpack .../libspeex1_1.2~rc1.2-1ubuntu1_amd64.deb ...\n",
      "Unpacking libspeex1:amd64 (1.2~rc1.2-1ubuntu1) ...\n",
      "Selecting previously unselected package libsoxr0:amd64.\n",
      "Preparing to unpack .../libsoxr0_0.1.2-1_amd64.deb ...\n",
      "Unpacking libsoxr0:amd64 (0.1.2-1) ...\n",
      "Selecting previously unselected package libswresample-ffmpeg1:amd64.\n",
      "Preparing to unpack .../libswresample-ffmpeg1_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libswresample-ffmpeg1:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libtheora0:amd64.\n",
      "Preparing to unpack .../libtheora0_1.1.1+dfsg.1-8_amd64.deb ...\n",
      "Unpacking libtheora0:amd64 (1.1.1+dfsg.1-8) ...\n",
      "Selecting previously unselected package libtwolame0:amd64.\n",
      "Preparing to unpack .../libtwolame0_0.3.13-1.2_amd64.deb ...\n",
      "Unpacking libtwolame0:amd64 (0.3.13-1.2) ...\n",
      "Selecting previously unselected package libva1:amd64.\n",
      "Preparing to unpack .../libva1_1.7.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libva1:amd64 (1.7.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libvorbis0a:amd64.\n",
      "Preparing to unpack .../libvorbis0a_1.3.5-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libvorbis0a:amd64 (1.3.5-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\n",
      "Preparing to unpack .../libvorbisenc2_1.3.5-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libvorbisenc2:amd64 (1.3.5-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libwavpack1:amd64.\n",
      "Preparing to unpack .../libwavpack1_4.75.2-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libwavpack1:amd64 (4.75.2-2ubuntu0.2) ...\n",
      "Selecting previously unselected package libwebp5:amd64.\n",
      "Preparing to unpack .../libwebp5_0.4.4-1_amd64.deb ...\n",
      "Unpacking libwebp5:amd64 (0.4.4-1) ...\n",
      "Selecting previously unselected package libx264-148:amd64.\n",
      "Preparing to unpack .../libx264-148_2%3a0.148.2643+git5c65704-1_amd64.deb ...\n",
      "Unpacking libx264-148:amd64 (2:0.148.2643+git5c65704-1) ...\n",
      "Selecting previously unselected package libx265-79:amd64.\n",
      "Preparing to unpack .../libx265-79_1.9-3_amd64.deb ...\n",
      "Unpacking libx265-79:amd64 (1.9-3) ...\n",
      "Selecting previously unselected package libxvidcore4:amd64.\n",
      "Preparing to unpack .../libxvidcore4_2%3a1.3.4-1_amd64.deb ...\n",
      "Unpacking libxvidcore4:amd64 (2:1.3.4-1) ...\n",
      "Selecting previously unselected package libzvbi-common.\n",
      "Preparing to unpack .../libzvbi-common_0.2.35-10_all.deb ...\n",
      "Unpacking libzvbi-common (0.2.35-10) ...\n",
      "Selecting previously unselected package libzvbi0:amd64.\n",
      "Preparing to unpack .../libzvbi0_0.2.35-10_amd64.deb ...\n",
      "Unpacking libzvbi0:amd64 (0.2.35-10) ...\n",
      "Selecting previously unselected package libavcodec-ffmpeg56:amd64.\n",
      "Preparing to unpack .../libavcodec-ffmpeg56_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavcodec-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../libavc1394-0_0.5.4-4_amd64.deb ...\n",
      "Unpacking libavc1394-0:amd64 (0.5.4-4) ...\n",
      "Selecting previously unselected package libass5:amd64.\n",
      "Preparing to unpack .../libass5_0.13.1-1_amd64.deb ...\n",
      "Unpacking libass5:amd64 (0.13.1-1) ...\n",
      "Selecting previously unselected package libbluray1:amd64.\n",
      "Preparing to unpack .../libbluray1_1%3a0.9.2-2_amd64.deb ...\n",
      "Unpacking libbluray1:amd64 (1:0.9.2-2) ...\n",
      "Selecting previously unselected package libgme0:amd64.\n",
      "Preparing to unpack .../libgme0_0.6.0-3ubuntu0.16.04.1_amd64.deb ...\n",
      "Unpacking libgme0:amd64 (0.6.0-3ubuntu0.16.04.1) ...\n",
      "Selecting previously unselected package libmodplug1:amd64.\n",
      "Preparing to unpack .../libmodplug1_1%3a0.8.8.5-2_amd64.deb ...\n",
      "Unpacking libmodplug1:amd64 (1:0.8.8.5-2) ...\n",
      "Selecting previously unselected package libssh-gcrypt-4:amd64.\n",
      "Preparing to unpack .../libssh-gcrypt-4_0.6.3-4.3ubuntu0.6_amd64.deb ...\n",
      "Unpacking libssh-gcrypt-4:amd64 (0.6.3-4.3ubuntu0.6) ...\n",
      "Selecting previously unselected package libavformat-ffmpeg56:amd64.\n",
      "Preparing to unpack .../libavformat-ffmpeg56_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavformat-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libavresample-ffmpeg2:amd64.\n",
      "Preparing to unpack .../libavresample-ffmpeg2_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavresample-ffmpeg2:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../libbs2b0_3.1.0+dfsg-2.2_amd64.deb ...\n",
      "Unpacking libbs2b0:amd64 (3.1.0+dfsg-2.2) ...\n",
      "Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../libflite1_2.0.0-release-1_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.0.0-release-1) ...\n",
      "Selecting previously unselected package libtbb2:amd64.\n",
      "Preparing to unpack .../libtbb2_4.4~20151115-0ubuntu3_amd64.deb ...\n",
      "Unpacking libtbb2:amd64 (4.4~20151115-0ubuntu3) ...\n",
      "Selecting previously unselected package libopencv-core2.4v5:amd64.\n",
      "Preparing to unpack .../libopencv-core2.4v5_2.4.9.1+dfsg-1.5ubuntu1.1_amd64.deb ...\n",
      "Unpacking libopencv-core2.4v5:amd64 (2.4.9.1+dfsg-1.5ubuntu1.1) ...\n",
      "Selecting previously unselected package libopencv-imgproc2.4v5:amd64.\n",
      "Preparing to unpack .../libopencv-imgproc2.4v5_2.4.9.1+dfsg-1.5ubuntu1.1_amd64.deb ...\n",
      "Unpacking libopencv-imgproc2.4v5:amd64 (2.4.9.1+dfsg-1.5ubuntu1.1) ...\n",
      "Selecting previously unselected package libpostproc-ffmpeg53:amd64.\n",
      "Preparing to unpack .../libpostproc-ffmpeg53_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libpostproc-ffmpeg53:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libswscale-ffmpeg3:amd64.\n",
      "Preparing to unpack .../libswscale-ffmpeg3_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libswscale-ffmpeg3:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libsodium18:amd64.\n",
      "Preparing to unpack .../libsodium18_1.0.8-5_amd64.deb ...\n",
      "Unpacking libsodium18:amd64 (1.0.8-5) ...\n",
      "Selecting previously unselected package libzmq5:amd64.\n",
      "Preparing to unpack .../libzmq5_4.1.4-7ubuntu0.1_amd64.deb ...\n",
      "Unpacking libzmq5:amd64 (4.1.4-7ubuntu0.1) ...\n",
      "Selecting previously unselected package libavfilter-ffmpeg5:amd64.\n",
      "Preparing to unpack .../libavfilter-ffmpeg5_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavfilter-ffmpeg5:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../libcaca0_0.99.beta19-2ubuntu0.16.04.1_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta19-2ubuntu0.16.04.1) ...\n",
      "Selecting previously unselected package libcdio13:amd64.\n",
      "Preparing to unpack .../libcdio13_0.83-4.2ubuntu1_amd64.deb ...\n",
      "Unpacking libcdio13:amd64 (0.83-4.2ubuntu1) ...\n",
      "Selecting previously unselected package libcdio-cdda1:amd64.\n",
      "Preparing to unpack .../libcdio-cdda1_0.83-4.2ubuntu1_amd64.deb ...\n",
      "Unpacking libcdio-cdda1:amd64 (0.83-4.2ubuntu1) ...\n",
      "Selecting previously unselected package libcdio-paranoia1:amd64.\n",
      "Preparing to unpack .../libcdio-paranoia1_0.83-4.2ubuntu1_amd64.deb ...\n",
      "Unpacking libcdio-paranoia1:amd64 (0.83-4.2ubuntu1) ...\n",
      "Selecting previously unselected package libdc1394-22:amd64.\n",
      "Preparing to unpack .../libdc1394-22_2.2.4-1_amd64.deb ...\n",
      "Unpacking libdc1394-22:amd64 (2.2.4-1) ...\n",
      "Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../libjack-jackd2-0_1.9.10+20150825git1ed50c92~dfsg-1ubuntu1_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.10+20150825git1ed50c92~dfsg-1ubuntu1) ...\n",
      "Selecting previously unselected package libflac8:amd64.\n",
      "Preparing to unpack .../libflac8_1.3.1-4_amd64.deb ...\n",
      "Unpacking libflac8:amd64 (1.3.1-4) ...\n",
      "Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../libsndfile1_1.0.25-10ubuntu0.16.04.3_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.0.25-10ubuntu0.16.04.3) ...\n",
      "Selecting previously unselected package libpulse0:amd64.\n",
      "Preparing to unpack .../libpulse0_1%3a8.0-0ubuntu3.15_amd64.deb ...\n",
      "Unpacking libpulse0:amd64 (1:8.0-0ubuntu3.15) ...\n",
      "Selecting previously unselected package libsdl1.2debian:amd64.\n",
      "Preparing to unpack .../libsdl1.2debian_1.2.15+dfsg1-3ubuntu0.1_amd64.deb ...\n",
      "Unpacking libsdl1.2debian:amd64 (1.2.15+dfsg1-3ubuntu0.1) ...\n",
      "Selecting previously unselected package libxcb-shape0:amd64.\n",
      "Preparing to unpack .../libxcb-shape0_1.11.1-1ubuntu1_amd64.deb ...\n",
      "Unpacking libxcb-shape0:amd64 (1.11.1-1ubuntu1) ...\n",
      "Selecting previously unselected package libavdevice-ffmpeg56:amd64.\n",
      "Preparing to unpack .../libavdevice-ffmpeg56_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavdevice-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libvdpau1:amd64.\n",
      "Preparing to unpack .../libvdpau1_1.1.1-3ubuntu1_amd64.deb ...\n",
      "Unpacking libvdpau1:amd64 (1.1.1-3ubuntu1) ...\n",
      "Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../ffmpeg_7%3a2.8.17-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking ffmpeg (7:2.8.17-0ubuntu0.1) ...\n",
      "Selecting previously unselected package libaacs0:amd64.\n",
      "Preparing to unpack .../libaacs0_0.8.1-1_amd64.deb ...\n",
      "Unpacking libaacs0:amd64 (0.8.1-1) ...\n",
      "Selecting previously unselected package libbdplus0:amd64.\n",
      "Preparing to unpack .../libbdplus0_0.1.2-1_amd64.deb ...\n",
      "Unpacking libbdplus0:amd64 (0.1.2-1) ...\n",
      "Selecting previously unselected package mesa-va-drivers:amd64.\n",
      "Preparing to unpack .../mesa-va-drivers_18.0.5-0ubuntu0~16.04.1_amd64.deb ...\n",
      "Unpacking mesa-va-drivers:amd64 (18.0.5-0ubuntu0~16.04.1) ...\n",
      "Selecting previously unselected package mesa-vdpau-drivers:amd64.\n",
      "Preparing to unpack .../mesa-vdpau-drivers_18.0.5-0ubuntu0~16.04.1_amd64.deb ...\n",
      "Unpacking mesa-vdpau-drivers:amd64 (18.0.5-0ubuntu0~16.04.1) ...\n",
      "Selecting previously unselected package ninja-build.\n",
      "Preparing to unpack .../ninja-build_1.5.1-0.1ubuntu1_amd64.deb ...\n",
      "Unpacking ninja-build (1.5.1-0.1ubuntu1) ...\n",
      "Selecting previously unselected package i965-va-driver:amd64.\n",
      "Preparing to unpack .../i965-va-driver_1.7.0-1_amd64.deb ...\n",
      "Unpacking i965-va-driver:amd64 (1.7.0-1) ...\n",
      "Selecting previously unselected package va-driver-all:amd64.\n",
      "Preparing to unpack .../va-driver-all_1.7.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking va-driver-all:amd64 (1.7.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package vdpau-driver-all:amd64.\n",
      "Preparing to unpack .../vdpau-driver-all_1.1.1-3ubuntu1_amd64.deb ...\n",
      "Unpacking vdpau-driver-all:amd64 (1.1.1-3ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Setting up libglib2.0-0:amd64 (2.48.2-0ubuntu4.8) ...\n",
      "Setting up libasyncns0:amd64 (0.8-5build1) ...\n",
      "Setting up libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-11build1) ...\n",
      "Setting up libgsm1:amd64 (1.0.13-4) ...\n",
      "Setting up libraw1394-11:amd64 (2.1.1-2) ...\n",
      "Setting up libiec61883-0:amd64 (1.2.0-0.2) ...\n",
      "Setting up libogg0:amd64 (1.3.2-1) ...\n",
      "Setting up libsamplerate0:amd64 (0.1.8-8) ...\n",
      "Setting up liborc-0.4-0:amd64 (1:0.4.25-1) ...\n",
      "Setting up libschroedinger-1.0-0:amd64 (1.0.11-2.1build1) ...\n",
      "Setting up libxv1:amd64 (2:1.0.10-1) ...\n",
      "Setting up libopenal-data (1:1.16.0-3) ...\n",
      "Setting up libopenal1:amd64 (1:1.16.0-3) ...\n",
      "Setting up libslang2:amd64 (2.3.0-2ubuntu1.1) ...\n",
      "Setting up libusb-1.0-0:amd64 (2:1.0.20-1) ...\n",
      "Setting up libavutil-ffmpeg54:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libmp3lame0:amd64 (3.99.5+repack1-9build1) ...\n",
      "Setting up libopenjpeg5:amd64 (1:1.5.2-3.1) ...\n",
      "Setting up libopus0:amd64 (1.1.2-1ubuntu1) ...\n",
      "Setting up libshine3:amd64 (3.1.0-4) ...\n",
      "Setting up libsnappy1v5:amd64 (1.1.3-2) ...\n",
      "Setting up libspeex1:amd64 (1.2~rc1.2-1ubuntu1) ...\n",
      "Setting up libsoxr0:amd64 (0.1.2-1) ...\n",
      "Setting up libswresample-ffmpeg1:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libtheora0:amd64 (1.1.1+dfsg.1-8) ...\n",
      "Setting up libtwolame0:amd64 (0.3.13-1.2) ...\n",
      "Setting up libva1:amd64 (1.7.0-1ubuntu0.1) ...\n",
      "Setting up libvorbis0a:amd64 (1.3.5-3ubuntu0.2) ...\n",
      "Setting up libvorbisenc2:amd64 (1.3.5-3ubuntu0.2) ...\n",
      "Setting up libwavpack1:amd64 (4.75.2-2ubuntu0.2) ...\n",
      "Setting up libwebp5:amd64 (0.4.4-1) ...\n",
      "Setting up libx264-148:amd64 (2:0.148.2643+git5c65704-1) ...\n",
      "Setting up libx265-79:amd64 (1.9-3) ...\n",
      "Setting up libxvidcore4:amd64 (2:1.3.4-1) ...\n",
      "Setting up libzvbi-common (0.2.35-10) ...\n",
      "Setting up libzvbi0:amd64 (0.2.35-10) ...\n",
      "Setting up libavcodec-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libavc1394-0:amd64 (0.5.4-4) ...\n",
      "Setting up libass5:amd64 (0.13.1-1) ...\n",
      "Setting up libbluray1:amd64 (1:0.9.2-2) ...\n",
      "Setting up libgme0:amd64 (0.6.0-3ubuntu0.16.04.1) ...\n",
      "Setting up libmodplug1:amd64 (1:0.8.8.5-2) ...\n",
      "Setting up libssh-gcrypt-4:amd64 (0.6.3-4.3ubuntu0.6) ...\n",
      "Setting up libavformat-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libavresample-ffmpeg2:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libbs2b0:amd64 (3.1.0+dfsg-2.2) ...\n",
      "Setting up libflite1:amd64 (2.0.0-release-1) ...\n",
      "Setting up libtbb2:amd64 (4.4~20151115-0ubuntu3) ...\n",
      "Setting up libopencv-core2.4v5:amd64 (2.4.9.1+dfsg-1.5ubuntu1.1) ...\n",
      "Setting up libopencv-imgproc2.4v5:amd64 (2.4.9.1+dfsg-1.5ubuntu1.1) ...\n",
      "Setting up libpostproc-ffmpeg53:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libswscale-ffmpeg3:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libsodium18:amd64 (1.0.8-5) ...\n",
      "Setting up libzmq5:amd64 (4.1.4-7ubuntu0.1) ...\n",
      "Setting up libavfilter-ffmpeg5:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libcaca0:amd64 (0.99.beta19-2ubuntu0.16.04.1) ...\n",
      "Setting up libcdio13:amd64 (0.83-4.2ubuntu1) ...\n",
      "Setting up libcdio-cdda1:amd64 (0.83-4.2ubuntu1) ...\n",
      "Setting up libcdio-paranoia1:amd64 (0.83-4.2ubuntu1) ...\n",
      "Setting up libdc1394-22:amd64 (2.2.4-1) ...\n",
      "Setting up libjack-jackd2-0:amd64 (1.9.10+20150825git1ed50c92~dfsg-1ubuntu1) ...\n",
      "Setting up libflac8:amd64 (1.3.1-4) ...\n",
      "Setting up libsndfile1:amd64 (1.0.25-10ubuntu0.16.04.3) ...\n",
      "Setting up libpulse0:amd64 (1:8.0-0ubuntu3.15) ...\n",
      "Setting up libsdl1.2debian:amd64 (1.2.15+dfsg1-3ubuntu0.1) ...\n",
      "Setting up libxcb-shape0:amd64 (1.11.1-1ubuntu1) ...\n",
      "Setting up libavdevice-ffmpeg56:amd64 (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libvdpau1:amd64 (1.1.1-3ubuntu1) ...\n",
      "Setting up ffmpeg (7:2.8.17-0ubuntu0.1) ...\n",
      "Setting up libaacs0:amd64 (0.8.1-1) ...\n",
      "Setting up libbdplus0:amd64 (0.1.2-1) ...\n",
      "Setting up mesa-va-drivers:amd64 (18.0.5-0ubuntu0~16.04.1) ...\n",
      "Setting up mesa-vdpau-drivers:amd64 (18.0.5-0ubuntu0~16.04.1) ...\n",
      "Setting up ninja-build (1.5.1-0.1ubuntu1) ...\n",
      "Setting up i965-va-driver:amd64 (1.7.0-1) ...\n",
      "Setting up va-driver-all:amd64 (1.7.0-1ubuntu0.1) ...\n",
      "Setting up vdpau-driver-all:amd64 (1.1.1-3ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Removing intermediate container 472d6d94733f\n",
      " ---> bd37d7334c4a\n",
      "Step 9/26 : RUN pip install --upgrade torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
      " ---> Running in ca55ed900376\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torchvision==0.7.0+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.7.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (5.9 MB)\n",
      "Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0+cu101) (1.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0+cu101) (1.19.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==0.7.0+cu101) (8.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->torchvision==0.7.0+cu101) (0.18.2)\n",
      "Installing collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.7.0\n",
      "    Uninstalling torchvision-0.7.0:\n",
      "      Successfully uninstalled torchvision-0.7.0\n",
      "Successfully installed torchvision-0.7.0+cu101\n",
      "Removing intermediate container ca55ed900376\n",
      " ---> ca015ff27008\n",
      "Step 10/26 : ENV FORCE_CUDA=\"1\"\n",
      " ---> Running in 68ddbc29a297\n",
      "Removing intermediate container 68ddbc29a297\n",
      " ---> e751cf5de22b\n",
      "Step 11/26 : ENV TORCH_CUDA_ARCH_LIST=\"Pascal;Volta;Turing\"\n",
      " ---> Running in 0a97d263f959\n",
      "Removing intermediate container 0a97d263f959\n",
      " ---> 700e6359f335\n",
      "Step 12/26 : ENV FVCORE_CACHE=\"/tmp\"\n",
      " ---> Running in 61b01389e47d\n",
      "Removing intermediate container 61b01389e47d\n",
      " ---> 222753e0c3ff\n",
      "Step 13/26 : WORKDIR /opt/ml/code\n",
      " ---> Running in 03672b2ba8b5\n",
      "Removing intermediate container 03672b2ba8b5\n",
      " ---> 4030268442d7\n",
      "Step 14/26 : RUN git clone https://github.com/open-mmlab/mmcv.git\n",
      " ---> Running in c95beac97ead\n",
      "\u001b[91mCloning into 'mmcv'...\n",
      "\u001b[0mRemoving intermediate container c95beac97ead\n",
      " ---> ba573fce1b03\n",
      "Step 15/26 : RUN cd mmcv && MMCV_WITH_OPS=1 pip install -e .\n",
      " ---> Running in a406bfac5ff4\n",
      "Obtaining file:///opt/ml/code/mmcv\n",
      "Collecting addict\n",
      "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from mmcv-full==1.3.0) (1.19.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from mmcv-full==1.3.0) (8.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from mmcv-full==1.3.0) (5.4.1)\n",
      "Collecting yapf\n",
      "  Downloading yapf-0.31.0-py2.py3-none-any.whl (185 kB)\n",
      "Installing collected packages: yapf, addict, mmcv-full\n",
      "  Running setup.py develop for mmcv-full\n",
      "Successfully installed addict-2.4.0 mmcv-full yapf-0.31.0\n",
      "Removing intermediate container a406bfac5ff4\n",
      " ---> e8de07c8f433\n",
      "Step 16/26 : RUN cd ..\n",
      " ---> Running in 6311ba45ef49\n",
      "Removing intermediate container 6311ba45ef49\n",
      " ---> ecde0fe7f30a\n",
      "Step 17/26 : RUN conda clean --all\n",
      " ---> Running in 0e5f7f54861d\n",
      "Cache location: \n",
      "There are no tarballs to remove\n",
      "WARNING: /root/.conda/pkgs does not exist\n",
      "Cache location: \n",
      "There are no unused packages to remove\n",
      "Removing intermediate container 0e5f7f54861d\n",
      " ---> 8e018a706be5\n",
      "Step 18/26 : RUN git clone https://github.com/open-mmlab/mmdetection.git mmdetection\n",
      " ---> Running in bc7909dc3e05\n",
      "\u001b[91mCloning into 'mmdetection'...\n",
      "\u001b[0mRemoving intermediate container bc7909dc3e05\n",
      " ---> 2c3ce387a293\n",
      "Step 19/26 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in 608676ccaba6\n",
      "Removing intermediate container 608676ccaba6\n",
      " ---> 6a18ad597986\n",
      "Step 20/26 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in 1f68c6d8f614\n",
      "Removing intermediate container 1f68c6d8f614\n",
      " ---> 802815ce0383\n",
      "Step 21/26 : ENV MMDETECTION /opt/ml/code/mmdetection\n",
      " ---> Running in 215052ad1000\n",
      "Removing intermediate container 215052ad1000\n",
      " ---> 20e00ded70e8\n",
      "Step 22/26 : RUN cd mmdetection/ &&     pip install -r /opt/ml/code/mmdetection/requirements/build.txt &&     pip install --no-cache-dir -e .\n",
      " ---> Running in 755264ef123e\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/code/mmdetection/requirements/build.txt (line 2)) (0.29.21)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/code/mmdetection/requirements/build.txt (line 3)) (1.19.1)\n",
      "Obtaining file:///opt/ml/code/mmdetection\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from mmdet==2.11.0) (3.3.4)\n",
      "Collecting mmpycocotools\n",
      "  Downloading mmpycocotools-12.0.3.tar.gz (23 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from mmdet==2.11.0) (1.19.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from mmdet==2.11.0) (1.15.0)\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->mmdet==2.11.0) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->mmdet==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->mmdet==2.11.0) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->mmdet==2.11.0) (8.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->mmdet==2.11.0) (2.8.1)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.6/site-packages (from mmpycocotools->mmdet==2.11.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.6/site-packages (from mmpycocotools->mmdet==2.11.0) (0.29.21)\n",
      "Building wheels for collected packages: mmpycocotools, terminaltables\n",
      "  Building wheel for mmpycocotools (setup.py): started\n",
      "  Building wheel for mmpycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for mmpycocotools: filename=mmpycocotools-12.0.3-cp36-cp36m-linux_x86_64.whl size=282850 sha256=2624031cead24f62b06b61f2a2ae2d01886cb163b8070d8383a638d17a8c036a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b_7o0bi1/wheels/79/e9/2f/545d78370691036bb34a3781984de6236882e367c97060b1ac\n",
      "  Building wheel for terminaltables (setup.py): started\n",
      "  Building wheel for terminaltables (setup.py): finished with status 'done'\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15354 sha256=2576e79478b2783eb01477975cb711a0abe962816f2ba2dc85c03b03e405ce1a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b_7o0bi1/wheels/86/1b/58/c23af2fe683acd8edc15d5a1268f0242be1ff2cf827fe34737\n",
      "Successfully built mmpycocotools terminaltables\n",
      "Installing collected packages: terminaltables, mmpycocotools, mmdet\n",
      "  Running setup.py develop for mmdet\n",
      "Successfully installed mmdet mmpycocotools-12.0.3 terminaltables-3.1.0\n",
      "Removing intermediate container 755264ef123e\n",
      " ---> 5c4f04617635\n",
      "Step 23/26 : COPY container_training/mmdetection_train.py /opt/ml/code\n",
      " ---> 897bde911b3a\n",
      "Step 24/26 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 3bd3c386dfd9\n",
      "Removing intermediate container 3bd3c386dfd9\n",
      " ---> f8cf1362cd95\n",
      "Step 25/26 : ENV SAGEMAKER_PROGRAM mmdetection_train.py\n",
      " ---> Running in b817a43be1b4\n",
      "Removing intermediate container b817a43be1b4\n",
      " ---> bfc9f5e12282\n",
      "Step 26/26 : WORKDIR /\n",
      " ---> Running in 2d94974e7625\n",
      "Removing intermediate container 2d94974e7625\n",
      " ---> 8772010992de\n",
      "Successfully built 8772010992de\n",
      "Successfully tagged mzanur-mmdetection-training:latest\n",
      "The push refers to repository [564829616587.dkr.ecr.us-east-1.amazonaws.com/mzanur-mmdetection-training]\n",
      "\n",
      "\u001b[1Bded756f4: Preparing \n",
      "\u001b[1Bae3727ff: Preparing \n",
      "\u001b[1B041dc3ba: Preparing \n",
      "\u001b[1Ba093bbcb: Preparing \n",
      "\u001b[1B867fc66f: Preparing \n",
      "\u001b[1B8ac591fc: Preparing \n",
      "\u001b[1Bd5839f9f: Preparing \n",
      "\u001b[1B487f2ecd: Preparing \n",
      "\u001b[1B9de3d1c7: Preparing \n",
      "\u001b[1Bc54d6636: Preparing \n",
      "\u001b[1Bbf0152ab: Preparing \n",
      "\u001b[1B56e55505: Preparing \n",
      "\u001b[1B73faf040: Preparing \n",
      "\u001b[1B78dab6ea: Preparing \n",
      "\u001b[1Bf1641cb6: Preparing \n",
      "\u001b[1B69267333: Preparing \n",
      "\u001b[1Bcd8033d8: Preparing \n",
      "\u001b[1Bf425b003: Preparing \n",
      "\u001b[1B9703999d: Preparing \n",
      "\u001b[1Ba0900494: Preparing \n",
      "\u001b[1B45bf29a5: Preparing \n",
      "\u001b[1B52375192: Preparing \n",
      "\u001b[1B83f1feee: Preparing \n",
      "\u001b[1B01cd20cc: Preparing \n",
      "\u001b[1B1c273442: Preparing \n",
      "\u001b[20B5839f9f: Waiting g \n",
      "\u001b[8Ba0900494: Waiting g \n",
      "\u001b[11B425b003: Waiting g \n",
      "\u001b[9B45bf29a5: Waiting g \n",
      "\u001b[20Bf0152ab: Waiting g \n",
      "\u001b[9B83f1feee: Waiting g \n",
      "\u001b[5B3594a68f: Waiting g \n",
      "\u001b[4B7e38f2d8: Waiting g \n",
      "\u001b[2B6d30da44: Waiting g \n",
      "\u001b[1B15d3a62b: Preparing \n",
      "\u001b[3Bd96b0113: Waiting g \n",
      "\u001b[1Bd2b930fc: Preparing \n",
      "\u001b[1Bec0db89a: Preparing \n",
      "\u001b[1B49baa658: Preparing \n",
      "\u001b[36B67fc66f: Pushed   274.6MB/274.4MB\u001b[36A\u001b[2K\u001b[37A\u001b[2K\u001b[38A\u001b[2K\u001b[38A\u001b[2K\u001b[38A\u001b[2K\u001b[40A\u001b[2K\u001b[37A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[39A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[38A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[38A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[33A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[38A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[32A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[30A\u001b[2K\u001b[25A\u001b[2K\u001b[30A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[31A\u001b[2K\u001b[18A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[5A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[32A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2Klatest: digest: sha256:cdf812e09ef5c0c082b6233e0ffee330c516090c44fa091ef9f7e4516db11b71 size: 8731\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh $container $tag Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "At training time, Sagemaker executes training script defined in `SAGEMAKER_PROGRAM` variable. In our case, this script does following\n",
    "- parses user parameters passed via Sagemaker Hyperparameter dictionary;\n",
    "- based on parameters constructs launch command;\n",
    "- uses `torch.distributed.launch` utility to launch distributed training;\n",
    "- uses MMDetection `tools/train.py` to configure trianing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmmcv\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Config\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_training_world\u001b[39;49;00m():\n",
      "\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Calculates number of devices in Sagemaker distributed cluster\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Get params of Sagemaker distributed cluster from predefined env variables\u001b[39;49;00m\n",
      "    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    num_cpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_CPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    hosts = json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    current_host = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Define PyTorch training world\u001b[39;49;00m\n",
      "    world = {}\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = num_gpus \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m num_cpus\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mlen\u001b[39;49;00m(hosts)\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmachine_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = hosts.index(current_host)\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmaster_addr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = hosts[\u001b[34m0\u001b[39;49;00m]\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmaster_port\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m55555\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# port is defined by Sagemaker\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m world\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtraining_configurator\u001b[39;49;00m(args, world):\n",
      "    \n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Configure training process by updating config file: \u001b[39;49;00m\n",
      "\u001b[33m    - takes base config from MMDetection templates;\u001b[39;49;00m\n",
      "\u001b[33m    - updates it with SageMaker specific data locations;\u001b[39;49;00m\n",
      "\u001b[33m    - overrides with user-defined options.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# updating path to config file inside SM container\u001b[39;49;00m\n",
      "    abs_config_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/code/mmdetection\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args.config_file)\n",
      "    cfg = Config.fromfile(abs_config_path)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.dataset.lower() == \u001b[33m\"\u001b[39;49;00m\u001b[33mcoco\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        \n",
      "        cfg.data_root = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[37m# By default, data will be download to /opt/ml/input/data/training\u001b[39;49;00m\n",
      "        cfg.data.train.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_train2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.train.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.val.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_val2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.val.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mval2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Note, that we are using validation dataset for testing purposes\u001b[39;49;00m\n",
      "        cfg.data.test.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_val2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.test.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mval2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Overriding config with options\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.options \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            cfg.merge_from_dict(args.options)\n",
      "        \n",
      "        \u001b[37m# scaling LR based on number of training processes\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.auto_scale:\n",
      "            cfg = auto_scale_config(cfg, world)\n",
      "        \n",
      "        updated_config = os.path.join(os.getcwd(), \u001b[33m\"\u001b[39;49;00m\u001b[33mupdated_config.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.dump(updated_config)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing config will be used for training:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcfg.pretty_text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDataset \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.dataset\u001b[33m}\u001b[39;49;00m\u001b[33m is not implemented.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                                    Currently only COCO-style datasets are available.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "              \n",
      "    \u001b[34mreturn\u001b[39;49;00m updated_config\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mauto_scale_config\u001b[39;49;00m(cfg, world):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Method automatically scales learning rate\u001b[39;49;00m\n",
      "\u001b[33m    based on number of processes in distributed cluster.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    When scaling, we take user-provided config as a config for single node with 8 GPUs\u001b[39;49;00m\n",
      "\u001b[33m    and scale it based on total number of training processes.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    Note, that batch size is not scaled, as MMDetection uses relative\u001b[39;49;00m\n",
      "\u001b[33m    batch size: cfg.data.samples_per_gpu\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    old_world_size = \u001b[34m8\u001b[39;49;00m \u001b[37m# Note, this is a hardcoded value, as MMDetection configs are build for single 8-GPU V100 node.\u001b[39;49;00m\n",
      "    old_lr = cfg.optimizer.lr\n",
      "    old_lr_warmup = cfg.lr_config.warmup_iters\n",
      "    scale = world[\u001b[33m\"\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] / old_world_size\n",
      "    \n",
      "    cfg.optimizer.lr = old_lr * scale\n",
      "    cfg.lr_config.warmup_iters = old_lr_warmup / scale\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mInitial learning rate \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mold_lr\u001b[33m}\u001b[39;49;00m\u001b[33m and warmup \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mold_lr_warmup\u001b[33m}\u001b[39;49;00m\u001b[33m were scaled \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m          to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcfg.optimizer.lr\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcfg.lr_config.warmup_iters\u001b[33m}\u001b[39;49;00m\u001b[33m respectively.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Each GPU has batch size of \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcfg.data.samples_per_gpu\u001b[33m}\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Total number of GPUs in training cluster is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mworld[\u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Effective batch size is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcfg.data.samples_per_gpu * world[\u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m cfg\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moptions_to_dict\u001b[39;49;00m(options):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Takes string of options in format of 'key1=value1; key2=value2 ...'\u001b[39;49;00m\n",
      "\u001b[33m    and produces dictionary object {'key1': 'value1', 'key2':'value2'...}.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    It also supports lists of values: key3=v1,v2,v3.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    options_dict = \u001b[36mdict\u001b[39;49;00m(item.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m options.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m; \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m options_dict.items():\n",
      "        value = [_parse_int_float_bool(v) \u001b[34mfor\u001b[39;49;00m v \u001b[35min\u001b[39;49;00m value.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)]\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(value) == \u001b[34m1\u001b[39;49;00m:\n",
      "            value = value[\u001b[34m0\u001b[39;49;00m]\n",
      "        options_dict[key] = value\n",
      "    \u001b[34mreturn\u001b[39;49;00m options_dict\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_int_float_bool\u001b[39;49;00m(val):\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mint\u001b[39;49;00m(val)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mfloat\u001b[39;49;00m(val)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m val.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfalse\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m val.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m val\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(config_path, work_dir, model_dir):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This method copies model trained weights and config \u001b[39;49;00m\n",
      "\u001b[33m    from output directory to model directory.\u001b[39;49;00m\n",
      "\u001b[33m    Sagemaker then automatically archives content of model directory\u001b[39;49;00m\n",
      "\u001b[33m    and adds it to model registry once training job is completed.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "\n",
      "    \u001b[37m# First copy config file\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        new_config_path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        shutil.copyfile(config_path, new_config_path)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mException when trying to copy \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig_path\u001b[33m}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnew_config_path\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(e)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Then copy checkpoints from work_dir\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m os.listdir(work_dir):\n",
      "        \u001b[34mif\u001b[39;49;00m file.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                checkpoint_path = os.path.join(work_dir, file)\n",
      "                new_checkpoint_path = os.path.join(model_dir, file)\n",
      "                shutil.copyfile(checkpoint_path, new_checkpoint_path)\n",
      "            \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mException when trying to copy \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcheckpoint_path\u001b[33m}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnew_checkpoint_path\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(e)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mModel config and checkpoints are saved to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Get initial configuration to select appropriate HuggingFace task and its configuration\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting training...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser = ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--config-file\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mFILE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOnly default MMDetection configs are supported now. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                        See for details: https://github.com/open-mmlab/mmdetection/tree/master/configs/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mcoco\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDefine which dataset format to use.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--options\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nargs=\u001b[33m'\u001b[39;49;00m\u001b[33m+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mConfig overrides.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--auto-scale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m s: s.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to scale batch parameters and learning rate based on cluster size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m s: s.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                    default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to scale batch parameters and learning rate based on cluster size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \n",
      "    args, unknown = parser.parse_known_args()\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.options \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        args.options = options_to_dict(args.options[\u001b[34m0\u001b[39;49;00m])        \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m unknown:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing arguments were not recognized and won\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt be used: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00munknown\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Derive parameters of distributed training cluster in Sagemaker\u001b[39;49;00m\n",
      "    world = get_training_world()  \n",
      "\n",
      "    \u001b[37m# Update config file\u001b[39;49;00m\n",
      "    config_file = training_configurator(args, world)\n",
      "              \n",
      "    \u001b[37m# Train script config\u001b[39;49;00m\n",
      "    launch_config = [ \u001b[33m\"\u001b[39;49;00m\u001b[33mpython -m torch.distributed.launch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--nnodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \u001b[33m\"\u001b[39;49;00m\u001b[33m--node_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mmachine_rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--nproc_per_node\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \u001b[33m\"\u001b[39;49;00m\u001b[33m--master_addr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, world[\u001b[33m'\u001b[39;49;00m\u001b[33mmaster_addr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--master_port\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, world[\u001b[33m'\u001b[39;49;00m\u001b[33mmaster_port\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      " \n",
      "    train_config = [os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMMDETECTION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mtools/train.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \n",
      "                    config_file, \n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m--launcher\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpytorch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m--work-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.validate:\n",
      "        train_config.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m--no-validate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Concat Pytorch Distributed Launch config and MMdetection config\u001b[39;49;00m\n",
      "    joint_cmd = \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(\u001b[36mstr\u001b[39;49;00m(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m launch_config+train_config)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing command will be executed: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, joint_cmd)\n",
      "    \n",
      "    process = subprocess.Popen(joint_cmd,  stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\n",
      "        output = process.stdout.readline()\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m process.poll() \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m output:\n",
      "            \u001b[36mprint\u001b[39;49;00m(output.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).strip())\n",
      "    rc = process.poll()\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m process.returncode != \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\n",
      "    \n",
      "    \u001b[37m# Before completing training, saving model artifacts\u001b[39;49;00m\n",
      "    save_model(config_file, os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    sys.exit(process.returncode)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "! pygmentize container_training/mmdetection_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "prefix_input = 'mmdetection-input'\n",
    "prefix_output = 'mmdetection-ouput'\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config-file\" : \"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\", # config path is relative to MMDetection root directory\n",
    "    \"dataset\" : \"coco\",\n",
    "    \"auto-scale\" : \"false\", # whether to scale LR and Warm Up time\n",
    "    \"validate\" : \"true\", # whether to run validation after training is done\n",
    "    \n",
    "    # 'options' allows to override individual config values\n",
    "    \"options\" : \"total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker will parse metrics from STDOUT and store/visualize them as part of training job\n",
    "metrics = [\n",
    "    {\n",
    "        \"Name\": \"loss\",\n",
    "        \"Regex\": \".*loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_cls\",\n",
    "        \"Regex\": \".*loss_rpn_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_bbox\",\n",
    "        \"Regex\": \".*loss_rpn_bbox:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_cls\",\n",
    "        \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"acc\",\n",
    "        \"Regex\": \".*acc:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_bbox\",\n",
    "        \"Regex\": \".*loss_bbox:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_mask\",\n",
    "        \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",\n",
    "        \"Regex\": \"lr: (-?\\d+.?\\d*(?:[Ee]-\\d+)?)\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training script and container locally\n",
    "\n",
    "\n",
    "Amazon SageMaker support [local mode](https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=local%20mode#local-mode) which allows you to deploy and run training job locally first, before deploying your training container to remote SageMaker Training cluster.\n",
    "\n",
    "To use local mode, we first need to install some dependencies. Please note, you may or may not need to restart your kernel for this changes to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker[local] in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.33.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.17.35)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.19.5)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (20.9)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (3.15.2)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (20.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25,!=1.25.1,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.26.3)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (5.4.1)\n",
      "Requirement already satisfied: docker-compose>=1.25.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker[local]) (1.28.6)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker[local]) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker[local]) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.35 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker[local]) (1.20.35)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.16.32->sagemaker[local]) (2.8.1)\n",
      "Requirement already satisfied: jsonschema<4,>=2.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (2.25.1)\n",
      "Requirement already satisfied: distro<2,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (1.5.0)\n",
      "Requirement already satisfied: cached-property<2,>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (1.5.1)\n",
      "Requirement already satisfied: python-dotenv<1,>=0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (0.15.0)\n",
      "Requirement already satisfied: websocket-client<1,>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (0.58.0)\n",
      "Requirement already satisfied: docker[ssh]<5,>=4.4.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (4.4.4)\n",
      "Requirement already satisfied: docopt<1,>=0.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (0.6.2)\n",
      "Requirement already satisfied: dockerpty<1,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (0.4.1)\n",
      "Requirement already satisfied: texttable<2,>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker-compose>=1.25.2->sagemaker[local]) (1.6.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (1.15.0)\n",
      "Requirement already satisfied: paramiko>=2.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (2.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker[local]) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker[local]) (3.7.4.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.25.2->sagemaker[local]) (0.17.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.25.2->sagemaker[local]) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker[local]) (2.4.7)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (1.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (3.4.4)\n",
      "Requirement already satisfied: cffi>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bcrypt>=3.1.3->paramiko>=2.4.2->docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4.2->docker[ssh]<5,>=4.4.4->docker-compose>=1.25.2->sagemaker[local]) (2.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->docker-compose>=1.25.2->sagemaker[local]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->docker-compose>=1.25.2->sagemaker[local]) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->docker-compose>=1.25.2->sagemaker[local]) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->sagemaker[local]) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "# Install all dependecies for local run. \n",
    "# Note you may need to restart your Sagemaker Notebook kernel to have changes applied.\n",
    "! pip install 'sagemaker[local]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "# Configure our local training session\n",
    "sagemaker_local_session = LocalSession()\n",
    "sagemaker_local_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training data locally\n",
    "! mkdir ../coco2017\n",
    "! aws s3 cp s3://mzanur-coco/complete/ ../coco2017 --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run our training container locally. For this, we need to pass special type of instance `local_gpu`. In this case, SageMaker will run training container with access to CUDA devices. Note, if you don't need access to GPUs, you may choose `local` instance type.\n",
    "\n",
    "Note, depending on configuration of your local host and available memory, you may run into memory issues when loading dataset. In this case, try reducing your batch size to bring down memory consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"sagemaker-local\" with the default driver\n",
      "Creating 50h3v521d6-algo-1-nqse7 ... \n",
      "Creating 50h3v521d6-algo-1-nqse7 ... done\n",
      "Attaching to 50h3v521d6-algo-1-nqse7\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,395 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,426 sagemaker-training-toolkit INFO     Failed to parse hyperparameter config-file value configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,427 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value coco to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,427 sagemaker-training-toolkit INFO     Failed to parse hyperparameter options value total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,449 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,451 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,482 sagemaker-training-toolkit INFO     Failed to parse hyperparameter config-file value configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,482 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value coco to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,482 sagemaker-training-toolkit INFO     Failed to parse hyperparameter options value total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,523 sagemaker-training-toolkit INFO     Failed to parse hyperparameter config-file value configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,524 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value coco to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,524 sagemaker-training-toolkit INFO     Failed to parse hyperparameter options value total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,565 sagemaker-training-toolkit INFO     Failed to parse hyperparameter config-file value configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,565 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value coco to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,565 sagemaker-training-toolkit INFO     Failed to parse hyperparameter options value total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True to Json.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Returning the value itself\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:45,576 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Training Env:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     },\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"current_host\": \"algo-1-nqse7\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"algo-1-nqse7\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     ],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"config-file\": \"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"dataset\": \"coco\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"auto-scale\": false,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"validate\": true,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"options\": \"total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     },\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"training\": {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         }\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     },\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"job_name\": \"mzanur-mmdetection-training-2021-04-09-00-17-42-800\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"master_hostname\": \"algo-1-nqse7\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"module_name\": \"mmdetection_train\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"num_gpus\": 4,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"current_host\": \"algo-1-nqse7\",\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             \"algo-1-nqse7\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     },\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     \"user_entry_point\": \"mmdetection_train.py\"\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m }\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Environment variables:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HOSTS=[\"algo-1-nqse7\"]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HPS={\"auto-scale\":false,\"config-file\":\"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\",\"dataset\":\"coco\",\"options\":\"total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true}\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_USER_ENTRY_POINT=mmdetection_train.py\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-nqse7\",\"hosts\":[\"algo-1-nqse7\"]}\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_CURRENT_HOST=algo-1-nqse7\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_MODULE_NAME=mmdetection_train\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_NUM_GPUS=4\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-nqse7\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-nqse7\"],\"hyperparameters\":{\"auto-scale\":false,\"config-file\":\"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\",\"dataset\":\"coco\",\"options\":\"total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"validate\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mzanur-mmdetection-training-2021-04-09-00-17-42-800\",\"log_level\":20,\"master_hostname\":\"algo-1-nqse7\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"mmdetection_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-nqse7\",\"hosts\":[\"algo-1-nqse7\"]},\"user_entry_point\":\"mmdetection_train.py\"}\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_USER_ARGS=[\"--auto-scale\",\"False\",\"--config-file\",\"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\",\"--dataset\",\"coco\",\"--options\",\"total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\",\"--validate\",\"True\"]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HP_CONFIG-FILE=configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HP_DATASET=coco\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HP_AUTO-SCALE=false\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HP_VALIDATE=true\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m SM_HP_OPTIONS=total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages:/opt/ml/code/mmcv:/opt/ml/code/mmdetection\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m /opt/conda/bin/python3.6 mmdetection_train.py --auto-scale False --config-file configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py --dataset coco --options total_epochs=12; optimizer.lr=0.08; evaluation.gpu_collect=True --validate True\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Starting training...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Following config will be used for training:model = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     type='MaskRCNN',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     pretrained='torchvision://resnet50',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     backbone=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='ResNet',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         depth=50,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         num_stages=4,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         out_indices=(0, 1, 2, 3),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         frozen_stages=1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         norm_cfg=dict(type='BN', requires_grad=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         norm_eval=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         style='pytorch'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     neck=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='FPN',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         in_channels=[256, 512, 1024, 2048],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         num_outs=5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     rpn_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='RPNHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         feat_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         anchor_generator=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='AnchorGenerator',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             scales=[8],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             ratios=[0.5, 1.0, 2.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             strides=[4, 8, 16, 32, 64]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         bbox_coder=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='DeltaXYWHBBoxCoder',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         loss_cls=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     roi_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='StandardRoIHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         bbox_roi_extractor=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='SingleRoIExtractor',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             featmap_strides=[4, 8, 16, 32]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         bbox_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='Shared2FCBBoxHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             fc_out_channels=1024,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             roi_feat_size=7,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             num_classes=80,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             bbox_coder=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='DeltaXYWHBBoxCoder',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             reg_class_agnostic=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             loss_cls=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         mask_roi_extractor=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='SingleRoIExtractor',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             featmap_strides=[4, 8, 16, 32]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         mask_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             type='FCNMaskHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             num_convs=4,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             conv_out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             num_classes=80,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             loss_mask=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     train_cfg=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         rpn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             assigner=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='MaxIoUAssigner',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 pos_iou_thr=0.7,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 neg_iou_thr=0.3,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 min_pos_iou=0.3,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 match_low_quality=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 ignore_iof_thr=-1),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             sampler=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='RandomSampler',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 num=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 pos_fraction=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 neg_pos_ub=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 add_gt_as_proposals=False),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             allowed_border=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             pos_weight=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             debug=False),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         rpn_proposal=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             nms_pre=2000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             max_per_img=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             nms=dict(type='nms', iou_threshold=0.7),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             min_bbox_size=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         rcnn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             assigner=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='MaxIoUAssigner',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 pos_iou_thr=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 neg_iou_thr=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 min_pos_iou=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 match_low_quality=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 ignore_iof_thr=-1),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             sampler=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='RandomSampler',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 num=512,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 pos_fraction=0.25,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 neg_pos_ub=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 add_gt_as_proposals=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             mask_size=28,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             pos_weight=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             debug=False)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     test_cfg=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         rpn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             nms_pre=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             max_per_img=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             nms=dict(type='nms', iou_threshold=0.7),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             min_bbox_size=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         rcnn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             score_thr=0.05,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             nms=dict(type='nms', iou_threshold=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             max_per_img=100,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             mask_thr_binary=0.5)))\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dataset_type = 'CocoDataset'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m data_root = '/opt/ml/input/data/training'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_norm_cfg = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m train_pipeline = [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='RandomFlip', flip_ratio=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='DefaultFormatBundle'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m test_pipeline = [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m data = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     samples_per_gpu=2,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     workers_per_gpu=2,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     train=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         '/opt/ml/input/data/training/annotations/instances_train2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         img_prefix='/opt/ml/input/data/training/train2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='RandomFlip', flip_ratio=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='DefaultFormatBundle'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='Collect',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     val=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         '/opt/ml/input/data/training/annotations/instances_val2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         img_prefix='/opt/ml/input/data/training/val2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     test=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         '/opt/ml/input/data/training/annotations/instances_val2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         img_prefix='/opt/ml/input/data/training/val2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m             dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                         to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                     dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m                 ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m         ]))\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m evaluation = dict(metric=['bbox', 'segm'], gpu_collect=True)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m optimizer = dict(type='SGD', lr=0.08, momentum=0.9, weight_decay=0.0001)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m optimizer_config = dict(grad_clip=None)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m lr_config = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     policy='step',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     warmup='linear',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     warmup_iters=500,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     warmup_ratio=0.001,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m     step=[8, 11])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m checkpoint_config = dict(interval=1)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dist_params = dict(backend='nccl')\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m log_level = 'INFO'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m load_from = None\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m resume_from = None\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m workflow = [('train', 1)]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m total_epochs = 12\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Following command will be executed: \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m  python -m torch.distributed.launch --nnodes 1 --node_rank 0 --nproc_per_node 4 --master_addr algo-1-nqse7 --master_port 55555 /opt/ml/code/mmdetection/tools/train.py /opt/ml/code/updated_config.py --launcher pytorch --work-dir /opt/ml/output/data\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m *****************************************\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m *****************************************\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:54,366 - mmdet - INFO - Environment info:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ------------------------------------------------------------\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m sys.platform: linux\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Python: 3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) [GCC 9.3.0]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m CUDA available: True\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m GPU 0,1,2,3: Tesla V100-SXM2-16GB\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m CUDA_HOME: /usr/local/cuda\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m NVCC: Cuda compilation tools, release 10.1, V10.1.243\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m GCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m PyTorch: 1.6.0\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m PyTorch compiling details: PyTorch built with:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - GCC 5.4\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - C++ Version: 201402\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - OpenMP 201307 (a.k.a. OpenMP 4.0)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - NNPACK is enabled\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - CPU capability usage: AVX2\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - CUDA Runtime 10.1\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - NVCC architecture flags: -gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_70,code=compute_70\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - CuDNN 7.6.5\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - Built with CuDNN 7.6.3\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - Magma 2.5.2\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format, FORCE_FALLBACK_CUDA_MPI=1, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m TorchVision: 0.7.0+cu101\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m OpenCV: 3.4.2\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m MMCV: 1.3.0\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m MMCV Compiler: GCC 5.4\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m MMCV CUDA Compiler: 10.1\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m MMDetection: 2.11.0+\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ------------------------------------------------------------\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:55,435 - mmdet - INFO - Distributed training: True\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:56,489 - mmdet - INFO - Config:\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m model = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MaskRCNN',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pretrained='torchvision://resnet50',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m backbone=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='ResNet',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m depth=50,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num_stages=4,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m out_indices=(0, 1, 2, 3),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m frozen_stages=1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m norm_cfg=dict(type='BN', requires_grad=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m norm_eval=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m style='pytorch'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m neck=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='FPN',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m in_channels=[256, 512, 1024, 2048],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num_outs=5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rpn_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='RPNHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m feat_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m anchor_generator=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='AnchorGenerator',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m scales=[8],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ratios=[0.5, 1.0, 2.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m strides=[4, 8, 16, 32, 64]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m bbox_coder=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='DeltaXYWHBBoxCoder',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loss_cls=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m roi_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='StandardRoIHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m bbox_roi_extractor=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='SingleRoIExtractor',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m featmap_strides=[4, 8, 16, 32]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m bbox_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Shared2FCBBoxHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fc_out_channels=1024,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m roi_feat_size=7,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num_classes=80,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m bbox_coder=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='DeltaXYWHBBoxCoder',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m reg_class_agnostic=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loss_cls=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mask_roi_extractor=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='SingleRoIExtractor',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m featmap_strides=[4, 8, 16, 32]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mask_head=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='FCNMaskHead',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num_convs=4,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m in_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m conv_out_channels=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num_classes=80,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loss_mask=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m train_cfg=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rpn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m assigner=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MaxIoUAssigner',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_iou_thr=0.7,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m neg_iou_thr=0.3,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m min_pos_iou=0.3,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m match_low_quality=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ignore_iof_thr=-1),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m sampler=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='RandomSampler',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num=256,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_fraction=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m neg_pos_ub=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m add_gt_as_proposals=False),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m allowed_border=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_weight=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m debug=False),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rpn_proposal=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m nms_pre=2000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m max_per_img=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m nms=dict(type='nms', iou_threshold=0.7),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m min_bbox_size=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rcnn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m assigner=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MaxIoUAssigner',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_iou_thr=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m neg_iou_thr=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m min_pos_iou=0.5,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m match_low_quality=True,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ignore_iof_thr=-1),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m sampler=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='RandomSampler',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m num=512,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_fraction=0.25,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m neg_pos_ub=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m add_gt_as_proposals=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mask_size=28,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pos_weight=-1,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m debug=False)),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m test_cfg=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rpn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m nms_pre=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m max_per_img=1000,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m nms=dict(type='nms', iou_threshold=0.7),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m min_bbox_size=0),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m rcnn=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m score_thr=0.05,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m nms=dict(type='nms', iou_threshold=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m max_per_img=100,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mask_thr_binary=0.5)))\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dataset_type = 'CocoDataset'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m data_root = '/opt/ml/input/data/training'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_norm_cfg = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m train_pipeline = [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='RandomFlip', flip_ratio=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='DefaultFormatBundle'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m test_pipeline = [\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m data = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m samples_per_gpu=2,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m workers_per_gpu=2,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m train=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m '/opt/ml/input/data/training/annotations/instances_train2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_prefix='/opt/ml/input/data/training/train2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='RandomFlip', flip_ratio=0.5),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='DefaultFormatBundle'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Collect',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m val=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m '/opt/ml/input/data/training/annotations/instances_val2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_prefix='/opt/ml/input/data/training/val2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m test=dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='CocoDataset',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ann_file=\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m '/opt/ml/input/data/training/annotations/instances_val2017.json',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_prefix='/opt/ml/input/data/training/val2017',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m pipeline=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='LoadImageFromFile'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='MultiScaleFlipAug',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m img_scale=(1333, 800),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m flip=False,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m transforms=[\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Resize', keep_ratio=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='RandomFlip'),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m type='Normalize',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m mean=[123.675, 116.28, 103.53],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m std=[58.395, 57.12, 57.375],\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m to_rgb=True),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Pad', size_divisor=32),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='ImageToTensor', keys=['img']),\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dict(type='Collect', keys=['img'])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m ]))\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m evaluation = dict(metric=['bbox', 'segm'], gpu_collect=True)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m optimizer = dict(type='SGD', lr=0.08, momentum=0.9, weight_decay=0.0001)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m optimizer_config = dict(grad_clip=None)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m lr_config = dict(\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m policy='step',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m warmup='linear',\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m warmup_iters=500,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m warmup_ratio=0.001,\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m step=[8, 11])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m checkpoint_config = dict(interval=1)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m dist_params = dict(backend='nccl')\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m log_level = 'INFO'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m load_from = None\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m resume_from = None\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m workflow = [('train', 1)]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m total_epochs = 12\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m work_dir = '/opt/ml/output/data'\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m gpu_ids = range(0, 4)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:57,042 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:17:57,042 - mmdet - INFO - Use load_from_torchvision loader\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 114MB/s] 00/97.8M [00:00<?, ?B/s]\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m NCCL version 2.4.8+cuda10.1\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:18:00,086 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m \n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=15.30s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=15.31s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=15.36s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=15.40s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m fatal: Not a git repository (or any of the parent directories): .git\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m loading annotations into memory...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=0.50s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=0.50s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=0.51s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m Done (t=0.51s)\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m creating index...\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m index created!\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:18:20,620 - mmdet - INFO - Start running, host: root@algo-1-nqse7, work_dir: /opt/ml/output/data\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:18:20,620 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:19:22,486 - mmdet - INFO - Epoch [1][50/14659]\tlr: 7.912e-03, eta: 2 days, 12:24:56, time: 1.237, data_time: 0.913, memory: 4014, loss_rpn_cls: 0.4358, loss_rpn_bbox: 0.1154, loss_cls: 0.9351, acc: 90.9424, loss_bbox: 0.0978, loss_mask: 0.7347, loss: 2.3187\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m INFO:mmdet:Epoch [1][50/14659]\tlr: 7.912e-03, eta: 2 days, 12:24:56, time: 1.237, data_time: 0.913, memory: 4014, loss_rpn_cls: 0.4358, loss_rpn_bbox: 0.1154, loss_cls: 0.9351, acc: 90.9424, loss_bbox: 0.0978, loss_mask: 0.7347, loss: 2.3187\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:19:38,693 - mmdet - INFO - Epoch [1][100/14659]\tlr: 1.590e-02, eta: 1 day, 14:06:46, time: 0.324, data_time: 0.045, memory: 4015, loss_rpn_cls: 0.1727, loss_rpn_bbox: 0.0864, loss_cls: 0.4893, acc: 94.1909, loss_bbox: 0.2005, loss_mask: 0.6798, loss: 1.6287\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m INFO:mmdet:Epoch [1][100/14659]\tlr: 1.590e-02, eta: 1 day, 14:06:46, time: 0.324, data_time: 0.045, memory: 4015, loss_rpn_cls: 0.1727, loss_rpn_bbox: 0.0864, loss_cls: 0.4893, acc: 94.1909, loss_bbox: 0.2005, loss_mask: 0.6798, loss: 1.6287\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:19:55,144 - mmdet - INFO - Epoch [1][150/14659]\tlr: 2.390e-02, eta: 1 day, 6:45:21, time: 0.329, data_time: 0.041, memory: 4312, loss_rpn_cls: 0.1464, loss_rpn_bbox: 0.0983, loss_cls: 0.5152, acc: 92.3301, loss_bbox: 0.2719, loss_mask: 0.6424, loss: 1.6742\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m INFO:mmdet:Epoch [1][150/14659]\tlr: 2.390e-02, eta: 1 day, 6:45:21, time: 0.329, data_time: 0.041, memory: 4312, loss_rpn_cls: 0.1464, loss_rpn_bbox: 0.0983, loss_cls: 0.5152, acc: 92.3301, loss_bbox: 0.2719, loss_mask: 0.6424, loss: 1.6742\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m 2021-04-09 00:20:10,862 - mmdet - INFO - Epoch [1][200/14659]\tlr: 3.189e-02, eta: 1 day, 2:53:41, time: 0.314, data_time: 0.036, memory: 4312, loss_rpn_cls: 0.1705, loss_rpn_bbox: 0.0962, loss_cls: 0.5239, acc: 92.2588, loss_bbox: 0.2811, loss_mask: 0.6227, loss: 1.6943\n",
      "\u001b[36m50h3v521d6-algo-1-nqse7 |\u001b[0m INFO:mmdet:Epoch [1][200/14659]\tlr: 3.189e-02, eta: 1 day, 2:53:41, time: 0.314, data_time: 0.036, memory: 4312, loss_rpn_cls: 0.1705, loss_rpn_bbox: 0.0962, loss_cls: 0.5239, acc: 92.2588, loss_bbox: 0.2811, loss_mask: 0.6227, loss: 1.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to delete: /tmp/tmp8dewqg0l/algo-1-nqse7 Please remove it manually.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-7a48b033fa60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"file:///home/ec2-user/SageMaker/coco2017\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# may need to increase shm for local mode - https://github.com/aws/sagemaker-python-sdk/issues/937\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# sudo service docker restart on AL1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mOnly\u001b[0m \u001b[0mmeaningful\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mjob_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mgenerates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mname\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0mexperiment_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExperiment\u001b[0m \u001b[0mmanagement\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mDictionary\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthree\u001b[0m \u001b[0moptional\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1422\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mexperiment_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExperiment\u001b[0m \u001b[0mmanagement\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mused\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcalled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                 \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mDictionary\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m                 \u001b[0mthree\u001b[0m \u001b[0moptional\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ExperimentName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TrialName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'TrialComponentDisplayName'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mprofiler_rule_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_rule_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0mprofiler_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                    role=role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='local_gpu',\n",
    "                                    output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                    metric_definitions = metrics,\n",
    "                                    hyperparameters = hyperparameters, \n",
    "                                    sagemaker_session=sagemaker_local_session\n",
    ")\n",
    "\n",
    "est.fit({\"training\" : \"file:///home/ec2-user/SageMaker/coco2017\"})\n",
    "# may need to increase shm for local mode - https://github.com/aws/sagemaker-python-sdk/issues/937\n",
    "# sudo service docker restart on AL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Sagemaker Training \n",
    "\n",
    "Now that we tested our training scrip and container locally, we are ready to run training job on disrtibuted SageMaker training cluster. Execute cell below to start training on Sagemaker. Note, that you have available parameters such as `instance_count` and `instance_type` to manage your training cluster configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateTrainingJob operation: No S3 objects found under S3 URL \"s3://coco2017-34sb3-east1/coco/\" given in input data source. Please ensure that the bucket exists in the selected region (us-east-1), that objects exist under that S3 prefix, and that the role \"arn:aws:iam::564829616587:role/mzanur-sagemaker\" has \"s3:ListBucket\" permissions on bucket \"coco2017-34sb3-east1\". Error message from S3: All access to this object has been disabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-3621393effaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"s3://coco2017-34sb3-east1/coco/\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mOnly\u001b[0m \u001b[0mmeaningful\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mjob_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mgenerates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mname\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0mexperiment_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExperiment\u001b[0m \u001b[0mmanagement\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mDictionary\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthree\u001b[0m \u001b[0moptional\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1422\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mexperiment_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExperiment\u001b[0m \u001b[0mmanagement\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mused\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcalled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                 \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mDictionary\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m                 \u001b[0mthree\u001b[0m \u001b[0moptional\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ExperimentName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TrialName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'TrialComponentDisplayName'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mprofiler_rule_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_rule_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0mprofiler_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateTrainingJob operation: No S3 objects found under S3 URL \"s3://coco2017-34sb3-east1/coco/\" given in input data source. Please ensure that the bucket exists in the selected region (us-east-1), that objects exist under that S3 prefix, and that the role \"arn:aws:iam::564829616587:role/mzanur-sagemaker\" has \"s3:ListBucket\" permissions on bucket \"coco2017-34sb3-east1\". Error message from S3: All access to this object has been disabled"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 2\n",
    "\n",
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                          role=role,\n",
    "                                          instance_count=instance_count,\n",
    "                                          instance_type=instance_type,\n",
    "                                          train_volume_size=100,\n",
    "                                          output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                          metric_definitions = metrics,\n",
    "                                          hyperparameters = hyperparameters, \n",
    "                                          sagemaker_session=session\n",
    ")\n",
    "\n",
    "est.fit({\"training\" : \"s3://mzanur-data/coco/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
